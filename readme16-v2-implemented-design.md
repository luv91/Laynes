# Regulatory Update Pipeline - Implementation Status

**Date:** January 10, 2026
**Version:** 2.0 (Implementation)
**Status:** Infrastructure Complete - Automation Pending

---

## Executive Summary

This document tracks the implementation status of the Regulatory Update Pipeline designed in `readme16-updateddesign-new-notices.md`. The core infrastructure is implemented and **proven working** - but full automation requires connecting the pieces.

### Quick Status

| Phase | Description | Status | Actually Working? |
|-------|-------------|--------|-------------------|
| 1 | Quick Fix - 2024 Four-Year Review | âœ… Complete | âœ… YES - Data imported |
| 2 | Temporal Tables (232/IEEPA) | âœ… Complete | âš ï¸ Models exist, need migrations |
| 3 | Watchers (FR, CBP, USITC) | âœ… Complete | âœ… YES - Polls 229 docs from FR |
| 4 | Document Pipeline | âœ… Complete | âœ… YES - Fetch/Render/Chunk work |
| 5 | RAG Extraction | âœ… Complete | âœ… YES - XML extracts 394 changes |
| 6 | Validation + Write Gate | âœ… Complete | âš ï¸ Code exists, not connected |
| 7 | UI Freshness Indicators | âœ… Complete | âœ… YES - API works |

---

## CRITICAL: Have We Moved From Static to Dynamic?

**Q: Are we listening for new Federal Register notices automatically?**

**A: NO - not yet.** The watchers exist and work when called, but there's no scheduler running them automatically.

**Q: Can we process PDF/XML documents?**

**A: YES!** Tested and proven:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TESTED: Federal Register Document 2024-21217 (Four-Year Review)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  XML Fetch:    357,627 bytes downloaded from federalregister.gov           â”‚
â”‚  XML Render:   1,744 lines extracted with line numbers                     â”‚
â”‚  XML Extract:  394 tariff changes extracted (DETERMINISTIC - no LLM)       â”‚
â”‚                                                                             â”‚
â”‚  Sample extracted changes:                                                  â”‚
â”‚    HTS 8507.90.40 â†’ 25% effective 2024-09-27 (battery parts)               â”‚
â”‚    HTS 8703.80.00 â†’ 100% effective 2024-09-27 (EVs)                        â”‚
â”‚    HTS 6307.90.9842 â†’ 25% effective 2024-09-27 (N95 respirators)           â”‚
â”‚                                                                             â”‚
â”‚  PDF Fetch:    465,024 bytes downloaded                                     â”‚
â”‚  PDF Render:   2,834 lines extracted via pdfplumber                        â”‚
â”‚  PDF Extract:  742 unique HTS codes found (needs LLM for structure)        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Q: Why isn't it fully automated?**

**A: Three missing connections:**

1. **No Scheduler** - Watchers need to run on cron (every 6 hours)
2. **No Auto-Commit** - Extracted changes don't write to DB automatically
3. **No Migrations** - New tables (section_232_rates, etc.) not created

---

## Proof: The Pipeline Actually Works

### Test 1: Watcher Polls Federal Register
```python
from app.watchers import FederalRegisterWatcher
from datetime import date

watcher = FederalRegisterWatcher()
docs = watcher.poll(since_date=date(2024, 9, 1))
# Result: Found 229 documents!
```

### Test 2: XML Extraction is DETERMINISTIC (No LLM Needed)
```python
# Federal Register XML has <GPOTABLE> elements - structured data!
# We parse them directly:

L0180: 8507.90.40 | Parts of lead-acid storage batteries | 25 | 2024
L0184: 8702.90.31 | Motor vehicles to transport 16+ persons | 100 | 2024
L0188: 8703.80.00 | Motor vehicles w/electric motor | 100 | 2024
L0191: 6307.90.9842 | Surgical N95 Respirators | 25 | 2024

# Extraction result: 394 CandidateChange objects with HTS, rate, effective date
```

### Test 3: PDF Rendering Works
```python
# Using pdfplumber to extract text from PDFs
# Output: Line-numbered canonical text for evidence tracking

L0001: === PAGE 1 ===
L0002: Federal Register/Vol. 89, No. 181/Wednesday, September 18, 2024/Notices
L0003: ... (2,834 lines total)

# Found: 742 unique HTS codes in rendered text
```

---

## What's Implemented

### Phase 1: Quick Fix (Complete)

**Goal:** Fix immediate data gap for 2024 Four-Year Review

**What was done:**
- Updated `section_301_hts_codes.csv` with new 9903.91.xx codes
- Added 2024 Four-Year Review strategic sectors (Medical, Semiconductors, EVs, Batteries, Critical Minerals)
- Fixed facemask (6307.90.98) rate from 7.5% to 50%

**Verification:**
```bash
# Query should now return 50% rate
pipenv run python -c "
from app.web import create_app
from app.web.db.models.tariff_tables import Section301Inclusion
app = create_app()
with app.app_context():
    inc = Section301Inclusion.query.filter_by(hts_8digit='63079098').first()
    print(f'Rate: {inc.duty_rate if inc else \"Not found\"}')
"
```

---

### Phase 2: Temporal Tables (Complete)

**Goal:** Enable rate changes over time with `effective_start`/`effective_end`

**Files Modified:**
- `app/web/db/models/tariff_tables.py`

**New Models:**

#### Section232Rate
```python
class Section232Rate(BaseModel):
    __tablename__ = "section_232_rates"

    hts_8digit = db.Column(db.String(10), nullable=False, index=True)
    material_type = db.Column(db.String(20), nullable=False)  # steel, aluminum, copper
    chapter_99_claim = db.Column(db.String(16), nullable=False)
    chapter_99_disclaim = db.Column(db.String(16))
    duty_rate = db.Column(db.Numeric(5, 4), nullable=False)

    # Country exception (UK 25% vs global 50%)
    country_code = db.Column(db.String(3), nullable=True)

    # Temporal validity
    effective_start = db.Column(db.Date, nullable=False)
    effective_end = db.Column(db.Date, nullable=True)

    # Audit trail
    source_doc_id = db.Column(db.String(36))
    evidence_id = db.Column(db.String(36))
```

#### IeepaRate
```python
class IeepaRate(BaseModel):
    __tablename__ = "ieepa_rates"

    program_type = db.Column(db.String(20), nullable=False)  # fentanyl, reciprocal
    country_code = db.Column(db.String(3))  # NULL=all, 'CHN', 'HKG', 'MAC'
    chapter_99_code = db.Column(db.String(16), nullable=False)
    duty_rate = db.Column(db.Numeric(5, 4), nullable=False)
    condition_type = db.Column(db.String(50))

    effective_start = db.Column(db.Date, nullable=False)
    effective_end = db.Column(db.Date, nullable=True)

    source_doc_id = db.Column(db.String(36))
```

**Key Features:**
- Temporal queries with `effective_start <= date AND (effective_end IS NULL OR effective_end > date)`
- UK rate exception for Section 232 (25% vs global 50%)
- `get_rate_as_of()` class method for temporal lookups

---

### Phase 3: Watchers (Complete)

**Goal:** Auto-detect new documents from official sources

**Files Created:**
```
app/watchers/
â”œâ”€â”€ __init__.py           # Exports all watchers
â”œâ”€â”€ base.py               # BaseWatcher, DiscoveredDocument
â”œâ”€â”€ federal_register.py   # Federal Register API watcher
â”œâ”€â”€ cbp_csms.py          # CBP CSMS scraper
â””â”€â”€ usitc.py             # USITC HTS watcher
```

#### FederalRegisterWatcher
```python
class FederalRegisterWatcher(BaseWatcher):
    """
    Polls Federal Register API for Section 301 / IEEPA notices.

    API: https://www.federalregister.gov/api/v1/documents.json

    Search queries:
    - "section 301 China tariff"
    - "9903.88" / "9903.91" (Chapter 99 codes)
    - "USTR modification"
    - "IEEPA fentanyl" / "IEEPA reciprocal"
    """

    SEARCH_QUERIES = [
        {"term": "section 301 China tariff", "agencies": ["trade-representative"]},
        {"term": "9903.88", "agencies": ["trade-representative"]},
        {"term": "9903.91", "agencies": ["trade-representative"]},
        {"term": "IEEPA fentanyl", "agencies": None},
        {"term": "IEEPA reciprocal tariff", "agencies": None},
    ]

    def poll(self, since_date: date) -> List[DiscoveredDocument]:
        # Polls API, returns documents to enqueue
```

#### CBPCSMSWatcher
```python
class CBPCSMSWatcher(BaseWatcher):
    """
    Scrapes CBP CSMS archive for Section 232 bulletins.

    Archive: https://www.cbp.gov/trade/csms/archive

    Keywords: section 232, steel, aluminum, copper, 9903
    """

    def poll(self, since_date: date) -> List[DiscoveredDocument]:
        # Scrapes HTML, follows attachments
```

#### USITCWatcher
```python
class USITCWatcher(BaseWatcher):
    """
    Monitors USITC for HTS updates.

    RESTStop API: https://hts.usitc.gov/reststop/search?keyword=XXXX
    """

    def verify_hts_code(self, hts_code: str) -> dict:
        # On-demand verification of specific HTS
```

**Usage:**
```python
from app.watchers import FederalRegisterWatcher

watcher = FederalRegisterWatcher()
documents = watcher.poll(since_date=date(2024, 9, 1))
# Returns List[DiscoveredDocument] with pdf_url, xml_url, metadata
```

---

### Phase 4: Document Processing Pipeline (Complete)

**Goal:** Full fetch â†’ store â†’ render â†’ chunk workflow

**Files Created:**

#### Models (`app/models/`)
```
app/models/
â”œâ”€â”€ __init__.py           # Exports all models
â”œâ”€â”€ document_store.py     # OfficialDocument, DocumentChunk
â”œâ”€â”€ evidence.py           # EvidencePacket
â””â”€â”€ ingest_job.py         # IngestJob with DB locking
```

##### OfficialDocument
```python
class OfficialDocument(BaseModel):
    __tablename__ = "official_documents"

    source = db.Column(db.String(50), nullable=False)  # federal_register, cbp_csms
    external_id = db.Column(db.String(100), nullable=False)

    # URLs
    pdf_url = db.Column(db.String(500))
    xml_url = db.Column(db.String(500))
    html_url = db.Column(db.String(500))

    # Raw content
    raw_bytes = db.Column(db.LargeBinary)
    content_hash = db.Column(db.String(64), nullable=False)  # SHA256
    content_type = db.Column(db.String(100))
    content_size = db.Column(db.Integer)

    # Rendered content
    canonical_text = db.Column(db.Text)  # Line-numbered: "L0001: content"

    # Metadata
    title = db.Column(db.String(500))
    publication_date = db.Column(db.Date)
    effective_date = db.Column(db.Date)

    # Status: fetched â†’ rendered â†’ chunked â†’ extracted â†’ validated â†’ committed
    status = db.Column(db.String(50), default="fetched")
```

##### DocumentChunk
```python
class DocumentChunk(BaseModel):
    __tablename__ = "document_chunks"

    document_id = db.Column(db.String(36), nullable=False, index=True)
    chunk_index = db.Column(db.Integer, nullable=False)

    # Position
    line_start = db.Column(db.Integer)
    line_end = db.Column(db.Integer)

    # Content
    text = db.Column(db.Text, nullable=False)
    token_count = db.Column(db.Integer)

    # Type
    chunk_type = db.Column(db.String(50))  # narrative, table, heading
    section_heading = db.Column(db.String(200))
```

##### IngestJob
```python
class IngestJob(BaseModel):
    __tablename__ = "ingest_jobs"

    source = db.Column(db.String(50), nullable=False)
    external_id = db.Column(db.String(100), nullable=False)
    url = db.Column(db.String(500))

    # Versioning
    content_hash = db.Column(db.String(64))
    parent_job_id = db.Column(db.String(36))
    processing_reason = db.Column(db.String(100))  # initial, correction, reparse

    # Status workflow
    status = db.Column(db.String(50), default="queued")

    # Results
    document_id = db.Column(db.String(36))
    changes_extracted = db.Column(db.Integer, default=0)
    changes_committed = db.Column(db.Integer, default=0)

    @classmethod
    def claim_next(cls, source_filter=None):
        """Claim next job with FOR UPDATE SKIP LOCKED."""
```

#### Workers (`app/workers/`)
```
app/workers/
â”œâ”€â”€ __init__.py           # Exports all workers
â”œâ”€â”€ fetch_worker.py       # Download and hash documents
â”œâ”€â”€ render_worker.py      # Convert to canonical text
â”œâ”€â”€ chunk_worker.py       # Semantic chunking
â”œâ”€â”€ extraction_worker.py  # Extract tariff changes
â”œâ”€â”€ validation_worker.py  # Validate extractions
â”œâ”€â”€ write_gate.py         # Final checks before commit
â””â”€â”€ pipeline.py           # Orchestrator
```

##### FetchWorker
```python
class FetchWorker:
    """Downloads documents, computes SHA256 hash, stores raw bytes."""

    def process_job(self, job: IngestJob) -> Optional[OfficialDocument]:
        # Downloads URL, computes hash, checks for duplicates
        # Stores in OfficialDocument with content_type detection
```

##### RenderWorker
```python
class RenderWorker:
    """Converts raw documents to canonical line-numbered text."""

    def process(self, doc: OfficialDocument, job: IngestJob = None) -> bool:
        # Detects content type, calls appropriate renderer

    def _render_xml(self, raw_bytes: bytes) -> str:
        # Parses Federal Register XML (<P>, <FP>, <GPOTABLE>, <HD>)
        # Output: "L0001: First line\nL0002: Second line"

    def _render_pdf(self, raw_bytes: bytes) -> str:
        # Uses pdfplumber for PDF extraction

    def _render_docx(self, raw_bytes: bytes) -> str:
        # Uses python-docx for DOCX extraction
```

##### ChunkWorker
```python
class ChunkWorker:
    """Splits rendered documents into chunks for RAG."""

    MIN_CHUNK_TOKENS = 100
    TARGET_CHUNK_TOKENS = 500
    MAX_CHUNK_TOKENS = 900

    def process(self, doc: OfficialDocument, job: IngestJob = None) -> int:
        # Returns number of chunks created
        # Respects semantic boundaries (sections, tables)
        # Tracks line numbers for evidence
```

---

### Phase 5: RAG Extraction (Complete)

**Goal:** Extract tariff changes from documents

**Files Created:**
- `app/workers/extraction_worker.py`

#### CandidateChange
```python
@dataclass
class CandidateChange:
    """A proposed tariff change extracted from a document."""

    document_id: str
    hts_code: str
    description: str = ""

    # Chapter 99 codes
    old_chapter_99_code: Optional[str] = None
    new_chapter_99_code: str = ""

    # Rate schedule (supports staged implementations)
    rate_schedule: List[RateScheduleEntry] = field(default_factory=list)

    # Simple rate (for single-rate changes)
    rate: Optional[Decimal] = None
    effective_date: Optional[date] = None

    # Program
    program: str = ""  # section_301, section_232_steel
    product_group: str = ""

    # Evidence
    evidence_quote: str = ""
    evidence_chunk_id: Optional[str] = None
    evidence_line_start: int = 0
    evidence_line_end: int = 0

    # Extraction method
    extraction_method: str = ""  # xml_table, llm_rag
```

#### RateScheduleEntry
```python
@dataclass
class RateScheduleEntry:
    """Single rate entry in a staged schedule."""
    rate: Decimal
    effective_start: date
    effective_end: Optional[date] = None
```

#### ExtractionWorker
```python
class ExtractionWorker:
    """Extracts tariff changes using deterministic XML parsing + RAG."""

    def extract_from_document(self, doc: OfficialDocument, job: IngestJob = None) -> List[CandidateChange]:
        # 1. Try deterministic XML extraction first (for <GPOTABLE>)
        # 2. Fall back to RAG extraction for narrative content

    def _extract_from_xml(self, doc: OfficialDocument) -> List[CandidateChange]:
        # Parses <GPOTABLE> elements directly - NO LLM NEEDED
        # Extracts HTS code, description, rate, timing from rows

    def _extract_from_rag(self, doc: OfficialDocument) -> List[CandidateChange]:
        # Placeholder for LLM-based extraction
        # Would use chunks with chunk_type='narrative'
```

---

### Phase 6: Validation + Write Gate (Complete)

**Goal:** Audit-grade verification before database writes

**Files Created:**
- `app/workers/validation_worker.py`
- `app/workers/write_gate.py`
- `app/models/evidence.py`

#### ValidationWorker
```python
class ValidationWorker:
    """Validates extracted changes against source documents."""

    def validate(self, candidate: CandidateChange, doc: OfficialDocument = None) -> ValidationResult:
        # Deterministic checks:
        # 1. HTS code exists in document
        # 2. Chapter 99 code exists (if specified)
        # 3. Rate exists (if specified)
        # 4. Evidence quote exists verbatim

    def _deterministic_validation(self, candidate, doc) -> ValidationResult:
        # Checks all variants of HTS code format
        # Returns confidence score based on checks passed
```

#### ValidationResult
```python
@dataclass
class ValidationResult:
    is_valid: bool
    confidence: float = 0.0
    reason: Optional[str] = None

    hts_found: bool = False
    chapter_99_found: bool = False
    rate_found: bool = False
    quote_verified: bool = False

    corrected_quote: Optional[str] = None
    corrected_lines: Optional[tuple] = None
```

#### WriteGate
```python
class WriteGate:
    """Final checkpoint before database write."""

    TIER_A_SOURCES = ["federal_register", "cbp_csms", "usitc"]
    TIER_A_DOMAINS = ["federalregister.gov", "govinfo.gov", "cbp.gov", "usitc.gov", "ustr.gov"]

    def check(self, candidate: CandidateChange, validation: ValidationResult, doc: OfficialDocument = None) -> WriteDecision:
        # MUST pass all checks:
        # 1. Source is Tier A
        # 2. Document hash exists
        # 3. Canonical text exists
        # 4. HTS code in document
        # 5. Rate OR Chapter 99 code in document
        # 6. Validation passed with confidence >= 0.5
```

#### EvidencePacket
```python
class EvidencePacket(BaseModel):
    """Stores proof linking DB changes to source document quotes."""

    __tablename__ = "evidence_packets"

    document_id = db.Column(db.String(36), nullable=False)
    document_hash = db.Column(db.String(64), nullable=False)

    # Location
    line_start = db.Column(db.Integer)
    line_end = db.Column(db.Integer)

    # Evidence content
    quote_text = db.Column(db.Text, nullable=False)
    context_before = db.Column(db.Text)
    context_after = db.Column(db.Text)

    # What it proves
    proves_hts_code = db.Column(db.String(12))
    proves_chapter_99 = db.Column(db.String(16))
    proves_rate = db.Column(db.Numeric(5, 4))
    proves_effective_date = db.Column(db.Date)

    # Validation
    validated_by = db.Column(db.String(50))  # write_gate, human
    validated_at = db.Column(db.DateTime)
    confidence_score = db.Column(db.Float)
```

#### DocumentPipeline
```python
class DocumentPipeline:
    """Orchestrates the full document processing workflow."""

    def process_job(self, job: IngestJob) -> dict:
        # Stage 1: Fetch
        # Stage 2: Render
        # Stage 3: Chunk
        # Stage 4: Extract
        # Stage 5: Validate
        # Stage 6: Commit (through write gate)

    def process_url(self, source: str, external_id: str, url: str) -> dict:
        # Direct URL processing

    def process_queue(self, max_jobs: int = 10) -> List[dict]:
        # Batch queue processing with DB locking

    def reprocess_document(self, doc_id: str) -> dict:
        # Reprocess existing document (for parser updates)
```

---

### Phase 7: UI Freshness Indicators (Complete)

**Goal:** Show data currency to users

**Files Created:**
- `app/services/__init__.py`
- `app/services/freshness.py`

**Files Modified:**
- `app/web/views/tariff_views.py`

#### FreshnessService
```python
class FreshnessService:
    """Tracks and reports data freshness for tariff programs."""

    DATA_SOURCES = {
        "section_301": {"name": "Section 301 (China)", "source": "Federal Register", "table": "section_301_rates"},
        "section_232": {"name": "Section 232 (Steel/Aluminum/Copper)", "source": "CBP CSMS", "table": "section_232_rates"},
        "ieepa_fentanyl": {"name": "IEEPA Fentanyl", "source": "Federal Register", "table": "ieepa_rates"},
        "mfn_base_rates": {"name": "MFN Base Rates", "source": "USITC HTS", "table": "hts_base_rates"},
    }

    def get_all_freshness(self) -> Dict[str, dict]:
        # Returns freshness info for all programs

    def get_program_freshness(self, program_id: str) -> dict:
        # Returns: name, source, last_updated, status, watcher_status, record_count
```

#### New API Endpoints
```
GET /tariff/freshness
    Returns freshness info for all data sources

GET /tariff/freshness/<program_id>
    Returns freshness info for specific program

POST /tariff/calculate
    Now includes "data_freshness" in response
```

#### UI Changes
- Added freshness indicator section to results card
- Color-coded dots: ğŸŸ¢ current, ğŸŸ¡ stale, ğŸ”´ outdated
- Shows program name and relative time ("2 days ago", "1 week ago")

---

## What's NOT Connected Yet (To Make It Fully Automated)

### The Gap: Working Components Not Wired Together

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  CURRENT STATE: "Car with all parts but not assembled"                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  âœ… Engine works (XML extraction extracts 394 changes)                       â•‘
â•‘  âœ… Fuel system works (watcher finds 229 documents)                          â•‘
â•‘  âœ… Chassis exists (validation + write gate code)                            â•‘
â•‘                                                                              â•‘
â•‘  âŒ Not connected: No ignition to start the engine automatically             â•‘
â•‘  âŒ Not connected: No fuel line from tank to engine                          â•‘
â•‘  âŒ Not connected: No exhaust pipe to DB                                     â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 1. No Scheduler (Watchers Don't Run Automatically)

**What exists:** `FederalRegisterWatcher.poll()` - works when called manually
**What's missing:** Cron job or APScheduler to call it every 6 hours

**To fix:**
```python
# Option A: Add to Railway startCommand
# "python -c 'from app.watchers import FederalRegisterWatcher; FederalRegisterWatcher().poll()'"

# Option B: APScheduler in app
from apscheduler.schedulers.background import BackgroundScheduler

scheduler = BackgroundScheduler()
scheduler.add_job(run_watchers, 'interval', hours=6)
scheduler.start()
```

### 2. No Auto-Commit (Extracted Changes Stay in Memory)

**What exists:** `ExtractionWorker` returns 394 `CandidateChange` objects
**What's missing:** Code to write them to `section_301_rates` table

**To fix:**
```python
# In pipeline.py, after validation passes:
for candidate in validated_candidates:
    Section301Rate.create(
        hts_8digit=candidate.hts_code,
        chapter_99_code=candidate.new_chapter_99_code,
        duty_rate=candidate.rate,
        effective_start=candidate.effective_date,
        source_doc_id=doc.id,
        evidence_id=evidence.id,
    )
```

### 3. No Migrations (New Tables Don't Exist in DB)

**What exists:** Model classes for `Section232Rate`, `IeepaRate`, `OfficialDocument`, etc.
**What's missing:** Actual database tables

**To fix:**
```bash
pipenv run flask db migrate -m "Add regulatory pipeline tables"
pipenv run flask db upgrade
```

### 4. LLM-Based RAG Extraction (Placeholder Only)

**What exists:** `_extract_from_rag()` returns empty list
**What works:** `_extract_from_xml()` parses structured Federal Register XML tables

**Why it matters:**
- Federal Register XML has `<GPOTABLE>` - deterministic parsing works! (394 changes extracted)
- PDFs without tables need LLM to understand structure
- Most FR notices ARE XML - so this covers the majority

**To fix (if needed for PDFs):**
```python
def _extract_from_rag(self, doc: OfficialDocument) -> List[CandidateChange]:
    # Use OpenAI/Anthropic to extract from narrative chunks
    for chunk in doc.chunks:
        if chunk.chunk_type == 'narrative':
            response = openai.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": EXTRACTION_PROMPT.format(chunk=chunk.text)}]
            )
            # Parse response into CandidateChange objects
```

---

## How PDF/XML Processing Actually Works

### XML Documents (Federal Register - PREFERRED)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FEDERAL REGISTER XML PROCESSING (DETERMINISTIC - NO LLM)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. FETCH                                                                   â”‚
â”‚     URL: federalregister.gov/documents/full_text/xml/2024/09/18/2024-21217  â”‚
â”‚     Result: 357,627 bytes of structured XML                                 â”‚
â”‚                                                                             â”‚
â”‚  2. PARSE XML STRUCTURE                                                     â”‚
â”‚     <GPOTABLE>                                                              â”‚
â”‚       <ROW>                                                                 â”‚
â”‚         <ENT>8507.90.40</ENT>           â† HTS Code                         â”‚
â”‚         <ENT>Parts of batteries</ENT>   â† Description                      â”‚
â”‚         <ENT>25</ENT>                   â† Rate (%)                         â”‚
â”‚         <ENT>2024</ENT>                 â† Effective Year                   â”‚
â”‚       </ROW>                                                                â”‚
â”‚     </GPOTABLE>                                                             â”‚
â”‚                                                                             â”‚
â”‚  3. RENDER TO CANONICAL TEXT                                                â”‚
â”‚     L0180: 8507.90.40 | Parts of lead-acid storage batteries | 25 | 2024   â”‚
â”‚     L0188: 8703.80.00 | Motor vehicles w/electric motor | 100 | 2024       â”‚
â”‚                                                                             â”‚
â”‚  4. EXTRACT (DETERMINISTIC)                                                 â”‚
â”‚     CandidateChange(hts_code="8507.90.40", rate=0.25, effective=2024-09-27)â”‚
â”‚     CandidateChange(hts_code="8703.80.00", rate=1.00, effective=2024-09-27)â”‚
â”‚     ... (394 total changes extracted)                                       â”‚
â”‚                                                                             â”‚
â”‚  5. VALIDATE                                                                â”‚
â”‚     âœ“ HTS 8507.90.40 found in document? YES                                â”‚
â”‚     âœ“ Rate 25 found in document? YES                                       â”‚
â”‚     âœ“ Evidence line: L0180                                                 â”‚
â”‚                                                                             â”‚
â”‚  6. WRITE GATE (if connected)                                              â”‚
â”‚     âœ“ Tier A source? YES (federalregister.gov)                             â”‚
â”‚     âœ“ Hash verified? YES (SHA256 stored)                                   â”‚
â”‚     âœ“ Confidence >= 0.5? YES (1.0 for deterministic)                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PDF Documents (Fallback When No XML)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PDF PROCESSING (USES pdfplumber - LLM OPTIONAL)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. FETCH                                                                   â”‚
â”‚     URL: govinfo.gov/content/pkg/FR-2024-09-18/pdf/2024-21217.pdf          â”‚
â”‚     Result: 465,024 bytes                                                   â”‚
â”‚                                                                             â”‚
â”‚  2. EXTRACT TEXT (pdfplumber)                                               â”‚
â”‚     L0001: === PAGE 1 ===                                                   â”‚
â”‚     L0002: Federal Register/Vol. 89, No. 181/Wednesday...                   â”‚
â”‚     ... (2,834 lines)                                                       â”‚
â”‚                                                                             â”‚
â”‚  3. FIND HTS CODES (regex)                                                  â”‚
â”‚     Found: 742 unique HTS codes                                             â”‚
â”‚     8507.90.40, 8703.80.00, 6307.90.98, ...                                â”‚
â”‚                                                                             â”‚
â”‚  4. EXTRACT RATES (needs LLM for context)                                   â”‚
â”‚     âš ï¸ PDF tables aren't structured like XML                               â”‚
â”‚     âš ï¸ Need LLM to understand "25 percent" relates to which HTS            â”‚
â”‚     âš ï¸ This is the placeholder part                                        â”‚
â”‚                                                                             â”‚
â”‚  WHY XML IS PREFERRED:                                                      â”‚
â”‚  - XML has explicit <GPOTABLE> structure                                    â”‚
â”‚  - PDF is just rendered text - no structure                                 â”‚
â”‚  - Federal Register provides BOTH - we use XML when available              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## How Listening for New Notices Would Work (When Wired Up)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AUTOMATED FLOW (Not Yet Connected)                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  EVERY 6 HOURS:                                                             â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚  â”‚  SCHEDULER      â”‚  â† Missing: No cron/APScheduler                       â”‚
â”‚  â”‚  (cron job)     â”‚                                                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  WATCHER.poll()                                                 â”‚       â”‚
â”‚  â”‚                                                                  â”‚       â”‚
â”‚  â”‚  âœ… WORKING: Calls Federal Register API                        â”‚       â”‚
â”‚  â”‚  âœ… WORKING: Finds new documents since last poll               â”‚       â”‚
â”‚  â”‚  âœ… WORKING: Returns DiscoveredDocument objects                â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  CREATE INGEST JOB                                              â”‚       â”‚
â”‚  â”‚                                                                  â”‚       â”‚
â”‚  â”‚  âš ï¸ PARTIAL: IngestJob model exists                            â”‚       â”‚
â”‚  â”‚  âŒ MISSING: Auto-creation from watcher output                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  DOCUMENT PIPELINE                                               â”‚       â”‚
â”‚  â”‚                                                                  â”‚       â”‚
â”‚  â”‚  âœ… WORKING: Fetch (downloads, hashes)                         â”‚       â”‚
â”‚  â”‚  âœ… WORKING: Render (XML/PDF to canonical text)                â”‚       â”‚
â”‚  â”‚  âœ… WORKING: Chunk (semantic splitting)                        â”‚       â”‚
â”‚  â”‚  âœ… WORKING: Extract (394 changes from XML tables)             â”‚       â”‚
â”‚  â”‚  âœ… WORKING: Validate (checks HTS/rate in doc)                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  WRITE TO DATABASE                                               â”‚       â”‚
â”‚  â”‚                                                                  â”‚       â”‚
â”‚  â”‚  âŒ MISSING: Connect write_gate to Section301Rate.create()     â”‚       â”‚
â”‚  â”‚  âŒ MISSING: Database migrations for new tables                â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## To Make It Fully Automated: 3 Steps

### Step 1: Run Migrations
```bash
pipenv run flask db migrate -m "Add regulatory pipeline tables"
pipenv run flask db upgrade
```

### Step 2: Connect Write Gate to Database
```python
# In app/workers/pipeline.py, add after validation:
from app.web.db.models.tariff_tables import Section301Rate

def _commit_to_database(self, candidate: CandidateChange, evidence: EvidencePacket):
    """Actually write the change to the database."""
    Section301Rate.create(
        hts_8digit=candidate.hts_code.replace(".", "")[:8],
        chapter_99_code=candidate.new_chapter_99_code,
        duty_rate=candidate.rate,
        effective_start=candidate.effective_date,
        source_doc_id=str(candidate.document_id),
        evidence_id=str(evidence.id),
    )
    db.session.commit()
```

### Step 3: Add Scheduler
```python
# In app/__init__.py or separate worker process:
from apscheduler.schedulers.background import BackgroundScheduler
from app.watchers import FederalRegisterWatcher
from app.workers import DocumentPipeline

def run_regulatory_update():
    """Run every 6 hours."""
    watcher = FederalRegisterWatcher()
    docs = watcher.poll(since_date=get_last_poll_date())

    pipeline = DocumentPipeline()
    for doc in docs:
        pipeline.process_url(
            source=doc.source,
            external_id=doc.external_id,
            url=doc.xml_url or doc.pdf_url
        )

scheduler = BackgroundScheduler()
scheduler.add_job(run_regulatory_update, 'interval', hours=6)
scheduler.start()
```

---

## File Structure (Implemented)

```
lanes/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ watchers/                    # âœ… IMPLEMENTED
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ federal_register.py
â”‚   â”‚   â”œâ”€â”€ cbp_csms.py
â”‚   â”‚   â””â”€â”€ usitc.py
â”‚   â”‚
â”‚   â”œâ”€â”€ workers/                     # âœ… IMPLEMENTED
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ fetch_worker.py
â”‚   â”‚   â”œâ”€â”€ render_worker.py
â”‚   â”‚   â”œâ”€â”€ chunk_worker.py
â”‚   â”‚   â”œâ”€â”€ extraction_worker.py
â”‚   â”‚   â”œâ”€â”€ validation_worker.py
â”‚   â”‚   â”œâ”€â”€ write_gate.py
â”‚   â”‚   â””â”€â”€ pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                      # âœ… IMPLEMENTED
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_store.py
â”‚   â”‚   â”œâ”€â”€ evidence.py
â”‚   â”‚   â””â”€â”€ ingest_job.py
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                    # âœ… IMPLEMENTED
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ freshness.py
â”‚   â”‚
â”‚   â””â”€â”€ web/
â”‚       â””â”€â”€ db/
â”‚           â””â”€â”€ models/
â”‚               â””â”€â”€ tariff_tables.py  # âœ… MODIFIED (Section232Rate, IeepaRate)
â”‚
â””â”€â”€ readme16-v2-implemented-design.md  # This document
```

---

## Testing the Pipeline

### Test Imports
```bash
pipenv run python -c "
from app.workers import FetchWorker, RenderWorker, ChunkWorker, ExtractionWorker, ValidationWorker, WriteGate, DocumentPipeline
from app.models import OfficialDocument, DocumentChunk, EvidencePacket, IngestJob
from app.watchers import FederalRegisterWatcher, CBPCSMSWatcher, USITCWatcher
from app.services.freshness import FreshnessService
print('All imports successful!')
"
```

### Test Watcher
```python
from app.watchers import FederalRegisterWatcher
from datetime import date

watcher = FederalRegisterWatcher()
docs = watcher.poll(since_date=date(2024, 9, 1))
print(f"Found {len(docs)} documents")
for doc in docs[:5]:
    print(f"  - {doc.external_id}: {doc.title[:60]}...")
```

### Test Pipeline (Manual URL)
```python
from app.workers import DocumentPipeline
from app.web import create_app

app = create_app()
with app.app_context():
    pipeline = DocumentPipeline()
    result = pipeline.process_url(
        source="federal_register",
        external_id="2024-21217",
        url="https://www.federalregister.gov/documents/full_text/xml/2024/09/18/2024-21217.xml"
    )
    print(result)
```

### Test Freshness API
```bash
curl http://localhost:5001/tariff/freshness | python -m json.tool
```

---

## Next Steps

1. **Run Migrations** - Create database tables for new models
2. **Test Full Pipeline** - Process a real Federal Register document end-to-end
3. **Implement LLM Extraction** - Add OpenAI/Anthropic for unstructured content
4. **Set Up Scheduler** - APScheduler for automated polling
5. **Add Monitoring** - Dashboard for pipeline health

---

*Document created: January 10, 2026*
*Implementation completed: January 10, 2026*
